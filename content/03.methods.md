## Methods

### Data Acquisition and Processing

#### Text Scraping

We scraped all text and metadata using the web-crawling framework Scrapy [@https://scrapy.org/] (version 2.4.1).
We created three independent scrapy web spiders to process the news text, news citations, and research article metadata.
News articles were defined as all articles from 2005 to 2020 that were designated as "News", "News-and-Views", "News-Feature", "Career-Column", "Career-Feature", "Technology-Feature", and "Toolbox".
Using the spider “target_year_crawl.py”, we scraped the title, author, and main text from all news articles.
We character normalized the main text by mapping visually identical Unicode codepoints to a single Unicode codepoint and stripping all non-Unicode characters.
Using an additional spider defined in “doi_crawl.py”, we scaped all citations within news articles.
For simplicity, we only considered citations with a DOI included in either text or a hyperlink in this spider.
Other possible forms of citations, e.g., titles, were not included.
The DOIs were then queried using the Springer API.
The spider “article_author_crawl.py” scraped all articles designated "Article" or "Letters" from all possible research articles.
We only scraped author names, author positions, and associated affiliations from research articles.
It should be noted that news article designations changed over time.

#### Springer API

Springer was chosen over other publishers for multiple reasons: 1) it is a large publisher, second only to Elsevier; 2) it covers mulptiple subjects, in contrast to PubMed; 3) it has a large daily query limit (5000/day); and 4) it provided more author affiliation information than found in Elsevier. 
We generated a comparative background set for supplemental analysis with the Springer API by obtaining author information for research articles cited in the news.
We selected a random set of articles to generate the Springer background set.
These articles were the first 200 English language "Journal" articles returned by the Springer API for each month, resulting in 2400 articles per year for 2005 through 2020.
To get the author information for the cited articles, we queried the Springer API using the scraped DOI.
For both API query types, the author names, positions, and affiliations for each publication were stored and are available in "all_author_country.tsv" and "all_author_fullname.tsv".
 
### Name Normalization
 
Names were cast to lowercase and processed using the R package humaniformat [@https://cran.r-project.org/web/packages/humaniformat/index.html].
humaniformat identifies if names are reversed (Lastname, Firstname), as well as identifies middle names.
Since many last or first authorships may be non-names, we additionally filtered out any identified names if they match any of the following terms: "consortium", "group", "initiative", "team", "collab*", "committee", "center", "program", "author", or "institute".
Furthermore, since many articles only contain first and last name initials, we remove any names with a "." or "-" and are less than four characters.
Finally, we only consider any remaining names of more than two characters.
We do this additional filtering to remove any web-scraping or coreNLP errors that only return partial names.
 
Since gender and origin predictions require the speaker’s full name, when possible, a longer name with minimal edit (Levenshtein) distance mentioned within the same article replaces the coreNLP predicted speaker.
The name with the smallest edit distance, where character deletions have zero cost, is defined as the matching name.
Character deletion was assigned a zero cost because we would like exact substring matches.
For example, the calculated cost, including a cost for character deletion, between John and John Steinberg is 10; without character deletion, it is 0.
Compared with the distance between John and Jane Doe, with character deletion cost, it is 7; without it is 2.
Furthermore, the identification of simultaneously cited and quoted speakers used this same name matching strategy.
 
### Bootstrap Estimations
 
For all analyses, we took 5000 bootstrap samples for each year independently.
We sampled with replacement of size equal to the cardinality of the complete set of interest.
Bootstrap estimates for equations @eq:quote - @eq:mention-news were done by sampling the denominator set.
 
 
### Gender Analysis

The quote extraction and attribution annotator from the coreNLP pipeline was employed to identify quotes and their associated speakers in the news article text.
In some cases, coreNLP could not identify an associated speaker’s name but instead assigned a gendered pronoun.
In these instances, we used the gender of the pronoun for the analysis.
The R package genderizeR [@https://github.com/kalimu/genderizeR], a wrapper for the genderize.io API, predicted the gender of authors and speakers.
We predicted a name as male using the first name with a minimum cutoff of 50%.
To reduce the number of queries made to genderize.io, a previously cached gender prediction from [@doi:10.1101/2020.04.14.927251] was also used and can be found in the file "genderize.tsv".
All first name predictions from this analysis are in the file "genderize_update.tsv".
To estimate the gender gap for the quote gender analyses, we used the proportion of total quotes, not quoted speakers.
We used the proportion of quotes to measure speaker participation instead of only the diversity of speakers.
The specific formulas for a single year are shown in equations @eq:quote and @eq:first-author.
We did not consider any names where no prediction could be made or quotes where neither speaker nor gendered pronoun was associated.
$$\textrm{Prop. Male Quotes} = \frac{|\textrm{Male Speaker Quotes}|} {|\textrm{Male or Female Speaker Quotes}|}$${#eq:quote}
 
$$\textrm{Prop. Male First Authors} = \frac{|\textrm{Male First Authors}|} {|\textrm{Male or Female First Authors}|}$${#eq:first-author}
 
### Name Origin Analysis

We used the same quoted speakers as described in the previous section for the name origin analysis.
In addition, we also take all authors cited in a Nature news article.
In contrast to the gender prediction, we need to use the full name to predict name origin.
We submitted all extracted full names to Wiki-2019LSTM [@doi:10.1101/2020.04.14.927251] to predict one of ten possible name origins: African, CelticEnglish, EastAsian, European, Greek, Hispanic, Jewish, ArabTurkPers, Nordic, and SouthAsia.
We set the highest probability origin as the resultant assignment.
Similar to the gender analyses, quote proportions were again directly compared against publication rates.
For citations, we calculated the proportion of overall news citations for a given year for each name origin, as exemplified in @eq:cite-origin to calculate the citation rate for first authors with a Greek name origin.
 
$$\textrm{Prop. Greek } 1^\textrm{st} \textrm{ Author Citations} = \frac{| \textrm{Cited } 1^\textrm{st} \textrm{ Authors w/Greek Name Orig}|} {| \textrm{Cited } 1^\textrm{st} \textrm{ Authors w/any Name Orig}|}$${#eq:cite-origin}

### Country Mention Proportions

We estimated the prevalence of a country’s mentions by including all identified organizations, countries, states, or provinces from coreNLP's named entity annotater.
We queried the resultant terms using OpenStreetMap [@https://www.openstreetmap.org] to identify the associated country with the term.
Country-associated terms identified by coreNLP may be ambiguous, causing OpenStreetMap to return incorrect locations.
Therefore, we count country mentions only if we find at least two unique country-associated terms in an article.
We calculate the mentioned rate as the proportion of country-specific mentions divided by the total articles for a particular year, as exemplified in @eq:mention for calculating the mentioned rate for Mexico.
 
$$\textrm{Prop. Mexico Mentions} = \frac{|\textrm{Articles with} \geq 2 \textrm{ unique Mexico-related terms}|} {|\textrm{All News Articles}|}$${#eq:mention}
 
### Country Citation Proportions

To identify the citation rate of a particular country, we processed all authors’ affiliations for a specific article.
Since the affiliations could be in multiple formats, we again used OpenStreetMap to identify the country affiliation.
Additionally, we considered all affiliations for a single author.
We calculated a countries' citation rate as the number of citations for a country divided by either the number of Nature research articles (@eq:mention-bg) or the total number of papers cited by news articles for that year (@eq:mention-news).
Shown below are example calculations for Colombia for a single year.
 
$$\textrm{Prop. Colombia Citations} = \frac{|\textrm{Articles with} \geq 1 \textrm{ Colombia affiliation}|} {|\textrm{All Research Articles}|}$${#eq:mention-bg}
 
$$\textrm{Prop. Colombia Citations} = \frac{|\textrm{Articles with} \geq 1 \textrm{ Colombia affiliation}|} {|\textrm{All Articles Cited in Nature News}|}$${#eq:mention-news}
 
### Divergent Word Identification

After calculating the citation and mention proportion for each country, we identified countries outlying in their comparative citation or mention rate.
Outlier detection was done by subtracting the citation and mention rates, then identifying which countries were in the top or bottom 5% from each year.
We only considered countries identified as either high citation (Set C) or high mention (Set M) across all years.
We did not consider any country that was in the top and bottom 5% in different years.
Additionally, we only considered a country if cited or mentioned five times in a single year.
Once we identified set C/M countries, we analyzed the word frequencies in all news articles where the set C/M country was mentioned but not cited.
We believe this would provide insight into content differences between set C/M countries.
Using the R package tidytext [@https://cran.r-project.org/web/packages/tidytext/index.html] we extracted tokens, removed stop words, and calculated the token frequencies across all articles.
We only consider tokens in set C/M articles if the token has been observed at least 100 times across all articles.
We then identify tokens that have the most significant ratio of usage between the two sets.
Since there are differences in the number of articles per country within each set, we calculated a token frequency within a set as the median frequency within each countries associated articles.
We calculated the resultant token ratio as the country normalized citation frequency to the country normalized mention frequency.
We assert that the term must be observed at least once in each set.
