## Methods

### Data Acquisition and Processing

#### Text Scraping

We scraped all text and metadata using the web-crawling framework Scrapy [@https://scrapy.org/] (version 2.4.1).
We created three independent scrapy web spiders to process the news text, news citations, and research article metadata.
News articles were defined as all articles from 2005 to 2020 that were designated as "News", "News-and-Views", "News-Feature", "Career-Column", "Career-Feature", "Technology-Feature", and "Toolbox".
Using the spider “target_year_crawl.py”, we scraped the title, author, and main text from all news articles.
We character normalized the main text by mapping visually identical Unicode codepoints to a single Unicode codepoint and stripping all non-Unicode characters.
Using an additional spider defined in “doi_crawl.py”, we scaped all citations within news articles.
For simplicity, we only considered citations with a DOI included in either text or a hyperlink in this spider.
Other possible forms of citations, e.g., titles, were not included.
The DOIs were then queried using the _Springer_ API.
The spider “article_author_crawl.py” scraped all articles designated "Article" or "Letters" from all possible research articles.
We only scraped author names, author positions, and associated affiliations from research articles.
It should be noted that news article designations changed over time.

#### coreNLP

After news articles were scraped and processed, the text was processed using the coreNLP pipeline [@doi:10.3115/v1/P14-5010] (version 4.2.0).
The main purpose for using coreNLP was to identify named entities related to countries and quoted speakers.
The full set of annotaters were: tokenize, ssplit, pos, lemma,ner, parse, coref, quote.
We used the "statistical" algorithm to perform coreference resolution.
All results were output to json format for further downstream processing.


#### _Springer_ API

_Springer_ was chosen over other publishers for multiple reasons: 1) it is a large publisher, second only to Elsevier; 2) it covers multiple subjects, in contrast to PubMed; 3) its API has a large daily query limit (5000/day); and 4) it provided more author affiliation information than found in Elsevier. 
We generated a comparative background set for supplemental analysis with the _Springer_ API by obtaining author information for research articles cited in the news.
We selected a random set of articles to generate the _Springer_ background set.
These articles were the first 200 English language "Journal" articles returned by the _Springer_ API for each month, resulting in 2400 articles per year for 2005 through 2020.
To get the author information for the cited articles, we queried the _Springer_ API using the scraped DOI.
For both API query types, the author names, positions, and affiliations for each publication were stored and are available in "all_author_country.tsv" and "all_author_fullname.tsv".
 
### Name Normalization
 
Names were cast to lowercase and processed using the R package humaniformat [@https://cran.r-project.org/web/packages/humaniformat/index.html].
humaniformat identifies if names are reversed (Lastname, Firstname), as well as identifies middle names.
Since many last or first authorships may be non-names, we additionally filtered out any identified names if they partially or fully match any of the following terms: "consortium", "group", "initiative", "team", "collab", "committee", "center", "program", "author", or "institute".
Furthermore, since many articles only contain first and last name initials, we remove any names less than four letters with a  "." or "-".
Finally, we only consider any remaining names of more than two characters.
We do this additional filtering to remove any web-scraping or coreNLP errors that only return partial names.
 
Since gender and origin predictions require the speaker’s full name, when possible, a longer name with minimal edit (Levenshtein) distance mentioned within the same article replaces the coreNLP predicted speaker.
The name with the smallest edit distance, where character deletions have zero cost, is defined as the matching name.
Character deletion was assigned a zero cost because we would like exact substring matches.
For example, the calculated cost, including a cost for character deletion, between John and John Steinberg is 10; without character deletion, it is 0.
Compared with the distance between John and Jane Doe, with character deletion cost, it is 7; without it is 2.
Furthermore, the identification of simultaneously cited and quoted speakers used this same name matching strategy.
 

### Gender Analysis

The quote extraction and attribution annotator from the coreNLP pipeline was employed to identify quotes and their associated speakers in the news article text.
In some cases, coreNLP could not identify an associated speaker’s name but instead assigned a gendered pronoun.
In these instances, we used the gender of the pronoun for the analysis.
The R package genderizeR [@doi:10.32614/rj-2016-002], a wrapper for the genderize.io API [@https://genderize.io/], predicted the gender of authors and speakers.
We predicted a name as male using the first name with a minimum cutoff of 50%.
To reduce the number of queries made to genderize.io, a previously cached gender prediction from [@doi:10.1101/2020.04.14.927251] was also used and can be found in the file "genderize.tsv".
All first name predictions from this analysis are in the file "genderize_update.tsv".
To estimate the gender gap for the quote gender analyses, we used the proportion of total quotes, not quoted speakers.
We used the proportion of quotes to measure speaker participation instead of only the diversity of speakers.
The specific formulas for a single year are shown in equations @eq:quote and @eq:first-author.
We did not consider any names where no prediction could be made or quotes where neither speaker nor gendered pronoun was associated.

$$\textrm{Prop. Male Quotes} = \frac{|\textrm{Male Speaker Quotes}|} {|\textrm{Male or Female Speaker Quotes}|}$${#eq:quote}
 
$$\textrm{Prop. Male First Authors} = \frac{|\textrm{Male First Authors}|} {|\textrm{Male or Female First Authors}|}$${#eq:first-author}
 
### Name Origin Analysis

We used the same quoted speakers as described in the previous section for the name origin analysis.
In addition, we also take all authors cited in a _Nature_ news article.
In contrast to the gender prediction, we need to use the full name to predict name origin.
We submitted all extracted full names to Wiki-2019LSTM [@doi:10.1101/2020.04.14.927251] to predict one of ten possible name origins: African, CelticEnglish, EastAsian, European, Greek, Hispanic, Jewish, ArabTurkPers, Nordic, and SouthAsia.
We set the highest probability origin as the resultant assignment.
Similar to the gender analyses, quote proportions were again directly compared against publication rates.
For citations, we calculated the proportion of overall news citations for a given year for each name origin, as exemplified in @eq:cite-origin to calculate the citation rate for first authors with a Greek name origin.
 
$$\textrm{Prop. Greek } 1^\textrm{st} \textrm{ Author Citations} = \frac{| \textrm{Cited } 1^\textrm{st} \textrm{ Authors w/Greek Name Orig}|} {| \textrm{Cited } 1^\textrm{st} \textrm{ Authors w/any Name Orig}|}$${#eq:cite-origin}

### Country Mention Proportions

We estimated the prevalence of a country’s mentions by including all identified organizations, countries, states, or provinces from coreNLP's named entity annotater.
We queried the resultant terms using OpenStreetMap [@https://www.openstreetmap.org] to identify the associated country with the term.
All terms that were identified in the text 25 or more times were visually inspected for correctness.
Hand-edited entries are denoted in the OpenStreetMap cache file "osm_cache.tsv" by the column "hand_edited".
Still, this only accounts for less than 5% of the total entries.
Furthermore, country-associated terms identified by coreNLP may be ambiguous, causing OpenStreetMap to return incorrect locations.
Therefore, we count country mentions only if we find at least two unique country-associated terms in an article.
We calculate the mentioned rate as the proportion of country-specific mentions divided by the total articles for a particular year, as exemplified in @eq:mention for calculating the mentioned rate for Mexico.
 
$$\textrm{Prop. Mexico Mentions} = \frac{|\textrm{Articles with} \geq 2 \textrm{ unique Mexico-related terms}|} {|\textrm{All News Articles}|}$${#eq:mention}
 
### Country Citation Proportions

To identify the citation rate of a particular country, we processed all authors’ affiliations for a specific article.
Since the affiliations could be in multiple formats, we again used OpenStreetMap to identify the country affiliation.
Additionally, we considered all affiliations for a single author.
We calculated a countries' citation rate as the number of citations for a country divided by either the number of _Nature_ research articles (@eq:mention-bg) or the total number of papers cited by news articles for that year (@eq:mention-news).
Shown below are example calculations for Colombia for a single year.
 
$$\textrm{Prop. CO Affil. in Nature} = \frac{|\textrm{Articles with} \geq 1 \textrm{ CO affil. in Nature}|} {|\textrm{All Nature Research Articles}|}$${#eq:mention-bg}
 
$$\textrm{Prop. CO Affil. Citations} = \frac{|\textrm{Cited Articles in News with} \geq 1 \textrm{ CO affil.}|} {|\textrm{All Articles Cited in News}|}$${#eq:mention-news}
 
### Divergent Word Identification

After calculating the citation and mention proportion for each country, we identified countries outlying in their comparative citation or mention rate.
Outlier detection was done by subtracting the citation and mention rates, then identifying which countries were in the top or bottom 5% from each year.
We only considered countries identified as either high citation (Set C) or high mention (Set M) across all years.
We did not consider any country that was in the top and bottom 5% in different years.
Additionally, we only considered a country if cited or mentioned five times in a single year.
Once we identified set C/M countries, we analyzed the word frequencies in all news articles where the set C or M country was mentioned but not cited.
We believe this would provide insight into content differences between set C and M countries.
Text from news articles in 2020 were not considered due to an excess of SARS-CoV-2 related terms.
Using the R package tidytext [@https://cran.r-project.org/web/packages/tidytext/index.html] we extracted tokens, removed stop words, and calculated the token frequencies across all articles.
We only consider tokens in set C or M articles if the token has been observed at least 100 times across all articles.
We then identify tokens that have the most significant ratio of usage between the two sets.
Since there are differences in the number of articles per country within each set, we calculated a token frequency within a set as the median frequency within each countries associated articles.
We calculated the resultant token ratio as the country normalized citation frequency to the country normalized mention frequency.
To avoid divide by zero errors, a pseudocount of 1 is added to both the numerator and denominator.
We assert that the term must be observed at least once in each set.

### Bootstrap Estimations
 
For all analyses related to equations @eq:quote - @eq:mention-news, we independently selected 5000 bootstrap samples for each year.
We sampled with replacement of size equal to the cardinality of the complete set of interest.
Bootstrap estimates for equations @eq:quote - @eq:mention-news were performed by sampling the denominator set.
The 5th, 50th, and 95th quantiles across the estimates are reported as the lower, middle, and upper bounds.
For the divergent word analysis, due to computational constraints, we only took 1000 bootstrap samples.
The bootstrap estimates were taken by subsampling the news articles with replacement, each time recalculating the country-normalized token frequencies within each country set (C and M).
After the normalized frequencies within each country set were calculated, we calculated the ratio between country sets for each subsample with a pseudocount of 1 in the numerator and denominator, (C+1)/(M+1).
Again, the 5th, 50th, and 95th quantiles across the estimates are reported as the lower, middle, and upper bounds.
